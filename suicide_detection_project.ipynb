{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spacy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# import itertools\n",
    "# import collections\n",
    "# import unidecode\n",
    "# import contractions as contract\n",
    "# import pkg_resources\n",
    "# from spellchecker import SpellChecker \n",
    "# from symspellpy import SymSpell,Verbosity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\\\Collage\\\\year 3\\\\S.1.3\\\\NLP\\\\suicide detection\\\\suicide data\\\\Suicide_Detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unmamed: 0\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class\n",
       "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
       "1  Am I weird I don't get affected by compliments...  non-suicide\n",
       "2  Finally 2020 is almost over... So I can never ...  non-suicide\n",
       "3          i need helpjust help me im crying so hard      suicide\n",
       "4  I’m so lostHello, my name is Adam (16) and I’v...      suicide"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem porter to return the word into its root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillter the data\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\(|D|P)',text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text += ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in tokenizer_porter(text) if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0  Ex Wife Threatening SuicideRecently I left my ...      1\n",
       "1  Am I weird I don't get affected by compliments...      0\n",
       "2  Finally 2020 is almost over... So I can never ...      0\n",
       "3          i need helpjust help me im crying so hard      1\n",
       "4  I’m so lostHello, my name is Adam (16) and I’v...      1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoder to class columnn \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    116037\n",
      "0    116037\n",
      "Name: class, dtype: int64\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: class, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check class distribution for balance\n",
    "print(df['class'].value_counts())\n",
    "print(df['class'].value_counts(normalize=True))\n",
    "\n",
    "sns.countplot(x=df['class'])\n",
    "plt.title('Original Dataset Class Distribution')\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we did the EDA and the Data pre processing but it didn't make a significant change in the acuracy so we keeped it hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lowercase\n",
    "# lowercase = df['text'].str.islower()\n",
    "# print(lowercase.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check extra whitespace\n",
    "# extra_whitespace = df['text'].str.match('\\s\\s+')\n",
    "# print(extra_whitespace.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check URL\n",
    "# url = df['text'].str.contains(\"http\")\n",
    "# print(url.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check mentions\n",
    "# mention = df['text'].str.match('@(\\w+)')\n",
    "# print(mention.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check hashtags\n",
    "# hashtag = df['text'].str.match('#(\\w+)')\n",
    "# print(hashtag.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check subreddit tag\n",
    "# subreddit = df['text'].str.match('r/(\\w+)')\n",
    "# print(subreddit.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check users tag\n",
    "# users = df['text'].str.match('u/(\\w+)')\n",
    "# print(users.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check special characters\n",
    "# special_characters = df['text'].str.match('[^0-9a-zA-Z]+')\n",
    "#print(special_characters.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove more stop words and do bigram\n",
    "\n",
    "# df['without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# from nltk import bigrams\n",
    "# sentences = [text.split() for text in df['without_stopwords']]\n",
    "\n",
    "# # Create list of lists containing bigrams in tweets\n",
    "# terms_bigram = [list(bigrams(text)) for text in sentences]\n",
    "\n",
    "# # Flatten list of bigrams in clean tweets\n",
    "# bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# # Create counter of words in clean bigrams\n",
    "# bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "# #Create a table of the top 20 most paired words\n",
    "# bigram_df = pd.DataFrame(bigram_counts.most_common(20),\n",
    "#                             columns=['Bigram', 'Count'])\n",
    "\n",
    "# bigram_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining methods\n",
    "\n",
    "# vocab = collections.Counter()\n",
    "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "\n",
    "\n",
    "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "# sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# # Spell Check using Symspell\n",
    "# def fix_spelling(text):\n",
    "#     suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "#     correctedtext = suggestions[0].term # get the first suggestion, otherwise returns original text if nothing is corrected \n",
    "#     return correctedtext \n",
    "\n",
    "# # Remove some important words from stopwords list \n",
    "# deselect_stop_words = ['no', 'not']\n",
    "    \n",
    "# for w in deselect_stop_words:\n",
    "#     nlp.vocab[w].is_stop = False\n",
    "    \n",
    "# # Remove extra whitespaces from text\n",
    "# def remove_whitespace(text):\n",
    "#     text = text.strip()\n",
    "#     return \" \".join(text.split())\n",
    "\n",
    "# # Remove accented characters from text, e.g. café\n",
    "# def remove_accented_chars(text):\n",
    "#     text = unidecode.unidecode(text)\n",
    "#     return text\n",
    "\n",
    "# # Remove URL \n",
    "# def remove_url(text):\n",
    "#     return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "# # Removing symbols and digits\n",
    "# def remove_symbols_digits(text):\n",
    "#     return re.sub('[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "# # Removing special characters\n",
    "# def remove_special(text):\n",
    "#     return text.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"    \", \" \").replace('\"', '')\n",
    "\n",
    "# # Fix word lengthening (characters are wrongly repeated)\n",
    "# def fix_lengthening(text):\n",
    "#     pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "#     return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "# def text_preprocessing(text, accented_chars=True, contractions=True, convert_num=True, \n",
    "#                        extra_whitespace=True, lemmatization=True, lowercase=True, \n",
    "#                        url=True, symbols_digits=True, special_chars=True, \n",
    "#                        stop_words=True, lengthening=True, spelling=True):\n",
    "#     \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "#     if accented_chars == True: # remove accented characters\n",
    "#         text = remove_accented_chars(text)\n",
    "#     if contractions == True: # expand contractions\n",
    "#         text = contract.fix(text)\n",
    "#     if lowercase == True: # convert all characters to lowercase\n",
    "#         text = text.lower()\n",
    "#     if url == True: # remove URLs before removing symbols \n",
    "#         text = remove_url(text)\n",
    "#     if symbols_digits == True: # remove symbols and digits\n",
    "#         text = remove_symbols_digits(text)\n",
    "#     if special_chars == True: # remove special characters\n",
    "#         text = remove_special(text)\n",
    "#     if extra_whitespace == True: # remove extra whitespaces\n",
    "#         text = remove_whitespace(text)\n",
    "#     if lengthening == True: # fix word lengthening\n",
    "#         text = fix_lengthening(text)\n",
    "#     if spelling == True: # fix spelling\n",
    "#         text = fix_spelling(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using hashing vectorizer to vectorize the text data \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, \n",
    "                         preprocessor=None,tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the vectorizer on the train set and test set\n",
    "X_train = vect.transform(X_train)\n",
    "X_test = vect.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ML Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "log = LogisticRegression()\n",
    "\n",
    "log.fit(X_train, y_train)\n",
    "y_pred = log.predict(X_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "example = input(\"Enter your sentence:\") \n",
    "list1 = [example]\n",
    "X = vect.transform(list1 )\n",
    "print(\"Prediction:\",label[svm.predict(X)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"i will kill my self\"]\n",
    "X = vect.transform(example)\n",
    "print(\"Prediction:\",label[svm.predict(X)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"It's such a hot day, I'd like to have ice cream and visit the park\"]\n",
    "X = vect.transform(example)\n",
    "print(\"Prediction:\",label[svm.predict(X)[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "081206b255134db747542da7da2a0d35fb99d6042a7fb17ce92547f51a813e27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
